{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim =1)\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1604, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2060, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # 3 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.972\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2766e+01, -1.7961e+01, -1.4540e+01, -1.5774e+01, -2.4069e+01,\n",
      "        -2.1274e+01, -4.1184e+01, -1.3113e-06, -2.0611e+01, -1.4261e+01],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "a_featureset = X[0]\n",
    "reshaped_for_network = a_featureset.view(-1,784) # 784 b/c 28*28 image resolution.\n",
    "output = net(reshaped_for_network) #output will be a list of network predictions.\n",
    "first_pred = output[0]\n",
    "print(first_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "biggest_index = torch.argmax(first_pred)\n",
    "print(biggest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "REBUILD_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12501/12501 [05:01<00:00, 41.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12501/12501 [04:35<00:00, 45.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "net = Net().to()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21bb11300b8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[4], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "net = Net().to()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22452 2494\n"
     ]
    }
   ],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "print(len(train_X), len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [03:51<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.17425622045993805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [03:48<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.15940028429031372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [03:42<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.12670539319515228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [04:09<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Loss: 0.11114479601383209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [04:25<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Loss: 0.10114619135856628\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        \n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [02:32<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0482, grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [02:32<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0384, grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [02:32<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0434, grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [02:33<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0386, grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [02:36<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0369, grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        \n",
    "        matches = [torch.argmax(i)==torch.argmax(j) for i,j in zip (outputs, batch_y)]\n",
    "        in_sample_acc = matches.count(True)/len(matches)\n",
    "        \n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    \n",
    "    print(loss)\n",
    "    print(\"In-sample acc:\", round(in_sample_acc,2))\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.558\n"
     ]
    }
   ],
   "source": [
    "def batch_test(net):\n",
    "    BATCH_SIZE-100\n",
    "    correct = 0\n",
    "    total = 0\n",
    "with torch.no_grad():\n",
    "        batch_X = test_X[:BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_Y = test_y[:BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        \n",
    "        matches = [torch.argmax(i)==torch.argmax(j) for i,j in zip (outputs, batch_y)]\n",
    "        in_sample_acc = matches.count(True)/len(matches)\n",
    "        \n",
    "print(\"Accuracy: \", round(in_sample_acc, 3))\n",
    "batch_test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state dict:\n",
      "conv1.weight \t torch.Size([32, 1, 5, 5])\n",
      "conv1.bias \t torch.Size([32])\n",
      "conv2.weight \t torch.Size([64, 32, 5, 5])\n",
      "conv2.bias \t torch.Size([64])\n",
      "conv3.weight \t torch.Size([128, 64, 5, 5])\n",
      "conv3.bias \t torch.Size([128])\n",
      "fc1.weight \t torch.Size([512, 512])\n",
      "fc1.bias \t torch.Size([512])\n",
      "fc2.weight \t torch.Size([2, 512])\n",
      "fc2.bias \t torch.Size([2])\n",
      "Optimizer's state dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [2317757450064, 2317953022280, 2317953023432, 2317844096272, 2317844096200, 2317844095048, 2317844095408, 2317844098936, 2317844097496, 2317844095264]}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "print (\"Optimizer's state dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "torch.save(net.state_dict(), 'MyNet.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO. 1 dan 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.-zero_grad()  # sets gradients ke 0 sebelum kehilangan kalkulasi. Dilakukan untuk setiap step\n",
    "-no grad( ) #Ini dapat sangat membantu ketika mengevaluasi suatu model karena model tersebut mungkin memiliki parameter yang dapat dilatih dengan require_grad = True\n",
    "2.torch.flatten() digunakan untuk meratakan kisaran dims(gambar remang-remang) yang berdekatan dalam tensor.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
